{
  "name": "coderdata",
  "version": "2.0.2",
  "summary": "A package to download, load, and process multiple benchmark multi-omic drug response datasets",
  "description": "## Cancer Omics Drug Experiment Response Dataset \n\nThere is a recent explosion of deep learning algorithms that to tackle the computational problem of predicting drug treatment outcome from baseline molecular measurements. To support this,we have built a benchmark dataset that harmonizes diverse datasets to better assess algorithm performance.\n\nThis package collects diverse sets of paired molecular datasets with corresponding drug sensitivity data. All data here is reprocessed and standardized so it can be easily used as a benchmark dataset for the \nThis repository leverages existing datasets to collect the data\nrequired for deep learning model development. Since each deep learning model\nrequires distinct data capabilities, the goal of this repository is to\ncollect and format all data into a schema that can be leveraged for\nexisting models.\n\n![Coderdata Motivation](coderdata_overview.jpg?raw=true \"Motivation behind\ncoderdata develompent\")\n\n\nThe goal of this repository is two-fold: First, it aims to collate and\nstandardize the data for the broader community. This requires\nrunning a series of scripts to build and append to a standardized data\nmodel. Second, it has a series of scripts that pull from the data\nmodel to create model-specific data files that can be run by the data\ninfrastructure. \n\n## Data access\nFor the access to the latest version of CoderData, please visit our\n[documentation site](https://pnnl-compbio.github.io/coderdata/) which provides access to Figshare and\ninstructions for using the Python package to download the data.\n\n## Data format\nAll coderdata files are in text format - either comma delimited or tab\ndelimited (depending on data type). Each dataset can be evaluated\nindividually according to the CoderData schema that is maintained in [LinkML](schema/coderdata.yaml)\nand can be udpated via a commit to the repository. For more details,\nplease see the [schema description](schema/README.md).\n\n## Building a local version\n\nThe build process can be found in our [build\ndirectory](build/README.md). Here you can follow the instructions to\nbuild your own local copy of the data on your machine. \n\n## Adding a new dataset\n\nWe have standardized the build process so an additional dataset can be\nbuilt locally or as part of the next version of coder. Here are the\nsteps to follow:\n\n1. First visit the [build\ndirectory](build/README.md) and ensure you can build a local copy of\nCoderData. \n\n2. Checkout this repository and  create a subdirectory of the\n[build directory](build) with your own build files. \n\n3. Develop your scripts to build the data files according to our\n[LinkML Schema](schema/coderdata.yaml]). This will require collecting\nthe following metadata:\n- entrez gene identifiers (or you can use the `genes.csv` file\n- sample information such as species and model system type\n- drug name that can be searched on PubChem\n\nYou can validate each file by\nusing the [linkML\nvalidator](https://linkml.io/linkml/data/validating-data) together\nwith our schema file. \n\nYou can use the following scripts as part of your build process:\n- [build/utils/fit_curve.py](build/utils/fit_curve.py): This script\n  takes dose-response data and generates the dose-response statistics\n  required by CoderData/\n- [build/utils/pubchem_retrieval.py](build/utils/pubchem_retreival.py):\n  This script retreives structure and drug synonym information\n  required to populate the `Drug` table. \n\n4. Wrap your scripts in standard shell scripts with the following names\nand arguments:\n\n| shell script     | arguments                | description         |\n|------------------|--------------------------|---------------------|\n| `build_samples.sh` | [latest_samples] | Latest version of samples generated by coderdata build |\n| `build_omics.sh` | [gene file] [samplefile] | This includes the `genes.csv` that was generated in the original build as well as the sample file generated above. |\n| `build_drugs.sh` | [drugfile1,drugfile2,...]       | This includes a comma-delimited list of all drugs files generated from previous build  |\n| `build_exp.sh`| [samplfile ] [drugfile] | sample file and drug file generated by previous scripts |\n\n5. Put the Docker container file inside the [Docker\ndirectory](./build/docker) with the name\n`Dockerfile.[datasetname]`. \n\n6. Run `build_all.py` from the root directory, which should now add in\nyour Dockerfile in the mix and call the scripts in your Docker\ncontainer to build the files.\n\n\n",
  "author": null,
  "author_email": "Jeremy Jacobson <jeremy.jacobson@pnnl.gov>, Yannick Mahlich <yannick.mahlich@pnnl.gov>, Sara Gosline <sara.gosline@pnnl.gov>",
  "license": "2-clause BSD",
  "project_url": "https://pypi.org/project/coderdata/",
  "homepage": null,
  "requires_python": ">=3.9",
  "dependencies": [
    "numpy",
    "pandas",
    "pyyaml",
    "requests",
    "scikit-learn"
  ],
  "files": {
    "python": [
      "coderdata/_version.py",
      "coderdata/__init__.py",
      "coderdata/cli.py",
      "coderdata/dataset/__init__.py",
      "coderdata/dataset/dataset.py",
      "coderdata/utils/__init__.py",
      "coderdata/utils/utils.py",
      "coderdata/download/downloader.py",
      "coderdata/download/__init__.py"
    ],
    "documentation": [
      "README.md"
    ],
    "tests": [],
    "configuration": [
      "pyproject.toml"
    ],
    "data": [
      "coderdata/dataset.yml"
    ],
    "other": [
      "PKG-INFO",
      "LICENSE",
      ".gitignore"
    ]
  },
  "metadata": {
    "author": null,
    "author_email": "Jeremy Jacobson <jeremy.jacobson@pnnl.gov>, Yannick Mahlich <yannick.mahlich@pnnl.gov>, Sara Gosline <sara.gosline@pnnl.gov>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.11",
      "Programming Language :: Python :: 3.12",
      "Programming Language :: Python :: 3.13",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Bio-Informatics"
    ],
    "description": "## Cancer Omics Drug Experiment Response Dataset \n\nThere is a recent explosion of deep learning algorithms that to tackle the computational problem of predicting drug treatment outcome from baseline molecular measurements. To support this,we have built a benchmark dataset that harmonizes diverse datasets to better assess algorithm performance.\n\nThis package collects diverse sets of paired molecular datasets with corresponding drug sensitivity data. All data here is reprocessed and standardized so it can be easily used as a benchmark dataset for the \nThis repository leverages existing datasets to collect the data\nrequired for deep learning model development. Since each deep learning model\nrequires distinct data capabilities, the goal of this repository is to\ncollect and format all data into a schema that can be leveraged for\nexisting models.\n\n![Coderdata Motivation](coderdata_overview.jpg?raw=true \"Motivation behind\ncoderdata develompent\")\n\n\nThe goal of this repository is two-fold: First, it aims to collate and\nstandardize the data for the broader community. This requires\nrunning a series of scripts to build and append to a standardized data\nmodel. Second, it has a series of scripts that pull from the data\nmodel to create model-specific data files that can be run by the data\ninfrastructure. \n\n## Data access\nFor the access to the latest version of CoderData, please visit our\n[documentation site](https://pnnl-compbio.github.io/coderdata/) which provides access to Figshare and\ninstructions for using the Python package to download the data.\n\n## Data format\nAll coderdata files are in text format - either comma delimited or tab\ndelimited (depending on data type). Each dataset can be evaluated\nindividually according to the CoderData schema that is maintained in [LinkML](schema/coderdata.yaml)\nand can be udpated via a commit to the repository. For more details,\nplease see the [schema description](schema/README.md).\n\n## Building a local version\n\nThe build process can be found in our [build\ndirectory](build/README.md). Here you can follow the instructions to\nbuild your own local copy of the data on your machine. \n\n## Adding a new dataset\n\nWe have standardized the build process so an additional dataset can be\nbuilt locally or as part of the next version of coder. Here are the\nsteps to follow:\n\n1. First visit the [build\ndirectory](build/README.md) and ensure you can build a local copy of\nCoderData. \n\n2. Checkout this repository and  create a subdirectory of the\n[build directory](build) with your own build files. \n\n3. Develop your scripts to build the data files according to our\n[LinkML Schema](schema/coderdata.yaml]). This will require collecting\nthe following metadata:\n- entrez gene identifiers (or you can use the `genes.csv` file\n- sample information such as species and model system type\n- drug name that can be searched on PubChem\n\nYou can validate each file by\nusing the [linkML\nvalidator](https://linkml.io/linkml/data/validating-data) together\nwith our schema file. \n\nYou can use the following scripts as part of your build process:\n- [build/utils/fit_curve.py](build/utils/fit_curve.py): This script\n  takes dose-response data and generates the dose-response statistics\n  required by CoderData/\n- [build/utils/pubchem_retrieval.py](build/utils/pubchem_retreival.py):\n  This script retreives structure and drug synonym information\n  required to populate the `Drug` table. \n\n4. Wrap your scripts in standard shell scripts with the following names\nand arguments:\n\n| shell script     | arguments                | description         |\n|------------------|--------------------------|---------------------|\n| `build_samples.sh` | [latest_samples] | Latest version of samples generated by coderdata build |\n| `build_omics.sh` | [gene file] [samplefile] | This includes the `genes.csv` that was generated in the original build as well as the sample file generated above. |\n| `build_drugs.sh` | [drugfile1,drugfile2,...]       | This includes a comma-delimited list of all drugs files generated from previous build  |\n| `build_exp.sh`| [samplfile ] [drugfile] | sample file and drug file generated by previous scripts |\n\n5. Put the Docker container file inside the [Docker\ndirectory](./build/docker) with the name\n`Dockerfile.[datasetname]`. \n\n6. Run `build_all.py` from the root directory, which should now add in\nyour Dockerfile in the mix and call the scripts in your Docker\ncontainer to build the files.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": null,
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "dynamic": null,
    "home_page": null,
    "keywords": null,
    "license": "2-clause BSD",
    "license_expression": null,
    "license_files": [
      "LICENSE"
    ],
    "maintainer": null,
    "maintainer_email": null,
    "name": "coderdata",
    "package_url": "https://pypi.org/project/coderdata/",
    "platform": null,
    "project_url": "https://pypi.org/project/coderdata/",
    "project_urls": {
      "Documentation": "https://pnnl-compbio.github.io/coderdata/",
      "Homepage": "https://github.com/PNNL-CompBio/candleDataProcessing",
      "Issues": "https://github.com/PNNL-CompBio/coderdata/issues",
      "Repository": "https://github.com/PNNL-CompBio/coderdata.git"
    },
    "provides_extra": null,
    "release_url": "https://pypi.org/project/coderdata/2.0.2/",
    "requires_dist": [
      "numpy",
      "pandas",
      "pyyaml",
      "requests",
      "scikit-learn"
    ],
    "requires_python": ">=3.9",
    "summary": "A package to download, load, and process multiple benchmark multi-omic drug response datasets",
    "version": "2.0.2",
    "yanked": false,
    "yanked_reason": null
  }
}